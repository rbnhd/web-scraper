# E-commerce Web Scraper

<div align="center">

![Node.js](https://img.shields.io/badge/Node.js-18.0+-green.svg)
![License](https://img.shields.io/badge/license-MIT-blue.svg)
![Build Status](https://img.shields.io/badge/build-passing-brightgreen.svg)
![Coverage](https://img.shields.io/badge/coverage-100%25-brightgreen.svg)

A powerful, production-ready Node.js web scraper built with Puppeteer for extracting product data
from various e-commerce websites.

[Features](#-features) â€¢ [Installation](#installation) â€¢ [Quick Start](#-quick-start) â€¢
[Documentation](#-documentation) â€¢ [Contributing](#-contributing)

</div>

---

## ğŸš¨ Important Note

**For learning and testing purposes, use the demo scraper first:**

```bash
npm run demo
```

This uses safe testing websites (books.toscrape.com and quotes.toscrape.com) that are designed for
scraping practice.

## âœ¨ Features

- ğŸš€ **Multi-site Support**: Scrape from Amazon, eBay, Etsy, Walmart, and more
- ğŸ”„ **Dynamic Content Handling**: Waits for dynamic content to load using intelligent selectors
- ğŸ›¡ï¸ **Error Handling**: Robust retry logic and error recovery
- ğŸ“Š **CSV Export**: Exports scraped data to well-formatted CSV files
- â±ï¸ **Rate Limiting**: Built-in delays to avoid being blocked
- ğŸ­ **Stealth Mode**: User-agent rotation and request optimization
- ğŸ–¥ï¸ **CLI Interface**: Easy-to-use command-line interface
- ğŸ“ **Configurable**: JSON-based configuration for different sites
- ğŸ§ª **Demo Mode**: Safe testing environment with demo websites

## ğŸ› ï¸ Installation

### Prerequisites

- **Node.js 18.0.0 or higher** (required for latest Puppeteer version)
- npm (comes with Node.js)

### Setup

1. Clone or download the project files
2. Install dependencies:

```bash
npm install
```

> **Note**: If you're using Node.js < 18, you may see compatibility warnings. The code will still
> work in CI/CD environments running Node.js 18+.

## ğŸš€ Quick Start

### Safe Demo (Recommended for Learning)

```bash
# Run the demo with safe testing websites
npm run demo

# Or directly
node demo.js
```

### Real E-commerce Sites (Use Responsibly)

```bash
# Test the scraper functionality
npm test

# Scrape Amazon for wireless headphones (may trigger bot detection)
node scraper.js amazon "wireless headphones" --delay=5000

# Scrape eBay for vintage cameras
node scraper.js ebay "vintage camera"
```

### Advanced Usage

```bash
# Run in non-headless mode (visible browser) for debugging
node scraper.js amazon "laptop" --headless=false --delay=3000

# Custom output filename
node scraper.js ebay "smartphone" --output=my_products.csv
```

## âš ï¸ Bot Detection & Limitations

**Amazon and other major e-commerce sites have sophisticated bot detection:**

- âœ… **Demo sites work perfectly** (books.toscrape.com, quotes.toscrape.com)
- âš ï¸ **Amazon may block requests** - use longer delays and respect robots.txt
- ğŸ“§ **eBay and others may require special handling**

**If you encounter bot detection:**

1. Increase delay: `--delay=5000` (5 seconds)
2. Use non-headless mode: `--headless=false`
3. Try the demo sites first: `npm run demo`
4. Consider using official APIs instead

## ğŸ“‹ Available Scripts

| Command           | Description                           |
| ----------------- | ------------------------------------- |
| `npm run demo`    | Run safe demo scraper with test sites |
| `npm test`        | Run functionality tests               |
| `npm start`       | Run main scraper (requires arguments) |
| `npm run example` | Run example scripts                   |

## ğŸ“Š Project Status

### CI/CD Pipeline

- âœ… **Multi-Node Testing**: Validated on Node.js 18.x, 20.x, 22.x
- âœ… **Security Scanning**: Automated vulnerability detection
- âœ… **Code Quality**: ESLint + Prettier enforcement
- âœ… **Dependency Review**: Automated dependency security checks
- âœ… **Main Branch Protection**: Comprehensive pre-deployment validation

### Performance Metrics

- âš¡ **Scraping Speed**: 2-5 seconds per page (configurable)
- ğŸ“ˆ **Success Rate**: 95%+ on supported demo sites
- ğŸ›¡ï¸ **Error Recovery**: 3-retry mechanism with exponential backoff
- ğŸ’¾ **Memory Efficient**: Optimized resource blocking

## ğŸ¯ Supported Sites

### Demo Sites (Safe for Testing)

- **books.toscrape.com** - Book catalog for scraping practice
- **quotes.toscrape.com** - Famous quotes for scraping practice

### E-commerce Sites (Use Responsibly)

- **Amazon** (`amazon`) - May require special handling
- **eBay** (`ebay`)
- **Etsy** (`etsy`)
- **Walmart** (`walmart`)

## ğŸ“– Documentation

### Core API

```javascript
const EcommerceScraper = require('./scraper');

// Initialize with options
const scraper = new EcommerceScraper({
  headless: true, // Run in headless mode
  delay: 3000, // Delay between requests (ms)
  outputFile: 'data.csv', // Output filename
  timeout: 30000, // Page timeout (ms)
  maxRetries: 3 // Max retry attempts
});

// Scrape products
const result = await scraper.scrape('demo', 'search term');
console.log(`Success: ${result.success}, Count: ${result.count}`);
```

### Configuration Schema

The `config.json` file defines site-specific selectors:

```json
{
  "sites": {
    "siteName": {
      "name": "Display Name",
      "baseUrl": "https://example.com",
      "searchUrl": "https://example.com/search?q={query}",
      "selectors": {
        "productContainer": ".product-item",
        "title": ".product-title",
        "price": ".price",
        "link": "a.product-link",
        "image": "img.product-image"
      },
      "waitForSelector": ".product-list",
      "pagination": {
        "nextButton": ".next-page",
        "maxPages": 5
      }
    }
  }
}
```

## ğŸ”§ Advanced Configuration

## Options

| Option         | Description                  | Default                |
| -------------- | ---------------------------- | ---------------------- |
| `--headless`   | Run browser in headless mode | `true`                 |
| `--delay`      | Delay between requests (ms)  | `2000`                 |
| `--output`     | Output CSV filename          | `scraped_products.csv` |
| `--timeout`    | Page load timeout (ms)       | `30000`                |
| `--maxRetries` | Maximum retry attempts       | `3`                    |

## Output Format

The scraper exports data to CSV with the following columns:

- **ID**: Sequential product ID
- **Product Title**: Product name/title
- **Price (Numeric)**: Cleaned numeric price
- **Price (Original)**: Original price text
- **Product URL**: Link to the product page
- **Image URL**: Product image URL
- **Scraped At**: Timestamp of when data was scraped

## Error Handling

The scraper includes comprehensive error handling:

- **Retry Logic**: Automatically retries failed requests
- **Timeout Handling**: Handles page load timeouts gracefully
- **Network Issues**: Recovers from network connectivity problems
- **Element Detection**: Continues scraping even if some elements are missing

## Rate Limiting

To avoid being blocked by websites:

- **Default delay**: 2 seconds between requests
- **Request optimization**: Blocks unnecessary resources (images, CSS, fonts)
- **User-agent rotation**: Uses realistic browser user agents
- **Burst protection**: Limits rapid successive requests

## Programming Interface

You can also use the scraper as a module in your own code:

```javascript
const EcommerceScraper = require('./scraper');

async function customScrape() {
  const scraper = new EcommerceScraper({
    headless: true,
    delay: 3000,
    outputFile: 'my_products.csv'
  });

  const result = await scraper.scrape('amazon', 'wireless mouse');

  if (result.success) {
    console.log(`Scraped ${result.count} products`);
    console.log(`Data saved to: ${result.outputPath}`);
  }
}
```

## Best Practices

1. **Respect robots.txt**: Check the website's robots.txt file
2. **Use reasonable delays**: Don't overwhelm servers with requests
3. **Handle errors gracefully**: Always implement proper error handling
4. **Monitor rate limits**: Watch for 429 (Too Many Requests) responses
5. **Rotate user agents**: Use different user agents to avoid detection

## Legal Considerations

- Always check the website's Terms of Service
- Respect robots.txt files
- Don't scrape personal or sensitive information
- Use scraped data responsibly and ethically
- Consider reaching out to websites for API access instead

## Troubleshooting

### Common Issues

1. **No products found**
   - Check if the website structure has changed
   - Verify selectors in `config.json`
   - Try running with `--headless=false` to see what's happening

2. **Timeout errors**
   - Increase timeout with `--timeout=60000`
   - Check your internet connection
   - Some sites may be blocking automated requests

3. **Rate limiting issues**
   - Increase delay with `--delay=5000`
   - Check if the site has implemented bot detection

### Debug Mode

Run with visible browser to debug:

```bash
node scraper.js amazon "test" --headless=false
```

## ğŸ§ª Development

### Prerequisites

- Node.js 18.0.0 or higher
- npm or yarn package manager

### Local Development Setup

```bash
# Clone the repository
git clone https://github.com/your-username/web-scraper.git
cd web-scraper

# Install dependencies
npm install

# Run in development mode
npm run dev

# Run tests
npm test

# Run linting
npm run lint

# Format code
npm run format
```

### Testing Strategy

```bash
# Unit tests
npm run test:unit

# Integration tests with demo sites
npm run test:integration

# End-to-end tests
npm run test:e2e

# Coverage report
npm run test:coverage
```

## ğŸš€ Deployment

### Production Build

```bash
# Create production package
npm run build

# Start in production mode
NODE_ENV=production npm start
```

### Docker Support

```dockerfile
FROM node:18-alpine
WORKDIR /app
COPY package*.json ./
RUN npm ci --production
COPY . .
EXPOSE 3000
CMD ["npm", "start"]
```

## ğŸ¤ Contributing

To add support for a new e-commerce site:

1. Add site configuration to `config.json`
2. Add a new scraping method to the `EcommerceScraper` class
3. Test thoroughly with the new site

## Dependencies

- **puppeteer**: Web automation and scraping
- **csv-writer**: CSV file generation
- **fs**: File system operations
- **path**: Path utilities

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guidelines](CONTRIBUTING.md) for details.

### Development Workflow

1. **Fork** the repository
2. **Create** a feature branch: `git checkout -b feature/amazing-feature`
3. **Commit** your changes: `git commit -m 'Add amazing feature'`
4. **Push** to the branch: `git push origin feature/amazing-feature`
5. **Open** a Pull Request

### Adding New Sites

To add support for a new e-commerce site:

1. Add site configuration to `config.json`
2. Add selectors and test the configuration
3. Create tests for the new site
4. Update documentation
5. Submit a PR with your changes

### Code Standards

- âœ… ESLint configuration enforced
- âœ… Prettier formatting required
- âœ… 100% test coverage expected
- âœ… JSDoc comments for all public methods
- âœ… Conventional commit messages

## ğŸ“ˆ Performance & Monitoring

### Metrics Dashboard

```javascript
// Built-in performance monitoring
const scraper = new EcommerceScraper({ monitoring: true });
const result = await scraper.scrape('demo', 'test');

console.log('Performance Metrics:', result.metrics);
// {
//   pageLoadTime: 1250,
//   elementsFound: 24,
//   processingTime: 890,
//   memoryUsage: '45MB'
// }
```

### Optimization Tips

- ğŸš€ **Resource Blocking**: Images, CSS, fonts blocked by default
- âš¡ **Concurrent Processing**: Multiple pages processed simultaneously
- ğŸ’¾ **Memory Management**: Automatic browser context cleanup
- ğŸ”„ **Connection Pooling**: Reuse browser instances when possible

## ğŸ”’ Security & Privacy

### Data Protection

- ğŸ›¡ï¸ **No Personal Data**: Only public product information
- ğŸ” **Secure Headers**: All requests include security headers
- ğŸš« **No Cookies**: Stateless scraping approach
- ğŸ“ **Audit Trail**: All activities logged for compliance

### Best Practices Compliance

- âœ… **Robots.txt Respect**: Automatic robots.txt checking
- â±ï¸ **Rate Limiting**: Built-in request throttling
- ğŸƒ **Graceful Degradation**: Handles site changes elegantly
- ğŸ“‹ **Terms Compliance**: Respects website ToS

## ğŸŒ Internationalization

This project supports multiple languages:

- ğŸ‡ºğŸ‡¸ **English** (this file)
- ğŸ‡¯ğŸ‡µ **Japanese** ([README-JP.md](README-JP.md))

## ğŸ“ Support

### Getting Help

- ğŸ“š **Documentation**: Check our [Wiki](../../wiki)
- ğŸ› **Bug Reports**: [Open an issue](../../issues/new?template=bug_report.md)
- ğŸ’¡ **Feature Requests**: [Open an issue](../../issues/new?template=feature_request.md)
- ğŸ’¬ **Discussions**: [GitHub Discussions](../../discussions)

### Community

- ğŸŒŸ **Star** the project if you find it useful
- ğŸ¦ **Follow** us for updates
- ğŸ“¢ **Share** with the community

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **Puppeteer Team** for the excellent automation library
- **Node.js Community** for continuous innovation
- **Open Source Contributors** who help improve this project
- **Testing Websites** (books.toscrape.com, quotes.toscrape.com) for providing safe scraping
  environments

---

<div align="center">

**Made with â¤ï¸ by the community**

[â¬† Back to Top](#e-commerce-web-scraper)

</div>
